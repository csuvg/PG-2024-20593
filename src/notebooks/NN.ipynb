{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.357745Z",
     "start_time": "2024-09-29T08:43:56.425867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# Seed all possible\n",
    "seed_ = 2023\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)\n",
    "torch.manual_seed(seed_)\n",
    "\n",
    "# If using CUDA, you can set the seed for CUDA devices as well\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed_)\n",
    "    torch.cuda.manual_seed_all(seed_)\n",
    "    \n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False"
   ],
   "id": "5c199a17bf339c7f",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.481745Z",
     "start_time": "2024-09-29T08:43:58.358744Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../data/PROCESS/encoded_tch_prediction_data_zafrav3.2.csv')"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.497784Z",
     "start_time": "2024-09-29T08:43:58.482750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data.columns\n",
    "\n",
    "#data.drop(columns=['ABS_IDCOMP','ZAFRA'])"
   ],
   "id": "36a1ea8c9bdfe058",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.513033Z",
     "start_time": "2024-09-29T08:43:58.498762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Define features (X) and target variable (y)\n",
    "# X = data.drop('rendimiento', axis=1)\n",
    "# y = data['rendimiento']\n",
    "# \n",
    "# # Split the data into 80% training and 20% testing\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# \n",
    "# X_train, X_test, y_train, y_test = X_train.drop(columns=['ABS_IDCOMP','ZAFRA']), X_test.drop(columns=['ABS_IDCOMP','ZAFRA']), y_train.drop(columns=['ABS_IDCOMP','ZAFRA']), y_test.drop(columns=['ABS_IDCOMP','ZAFRA'])\n",
    "# print(\"80-20 Split:\")\n",
    "# print(f\"Training set shape: {X_train.shape}\")\n",
    "# print(f\"Testing set shape: {X_test.shape}\")"
   ],
   "id": "e1037cf38146464e",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.606760Z",
     "start_time": "2024-09-29T08:43:58.514003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data['ZAFRA'].hist()\n",
    "plt.show() "
   ],
   "id": "f74fba8bc5722117",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.637922Z",
     "start_time": "2024-09-29T08:43:58.608764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a mask for ZAFRA 22-23\n",
    "mask_22_23 = data['ZAFRA'] == '23-24'\n",
    "\n",
    "# Split the data\n",
    "X_train_zafra = data[~mask_22_23].drop('TCH', axis=1)\n",
    "y_train_zafra = data[~mask_22_23]['TCH']\n",
    "X_test_zafra = data[mask_22_23].drop('TCH', axis=1)\n",
    "y_test_zafra = data[mask_22_23]['TCH']\n",
    "\n",
    "X_train_zafra = X_train_zafra.drop(columns=['ABS_IDCOMP','ZAFRA'])\n",
    "y_train_zafra = y_train_zafra.drop(columns=['ABS_IDCOMP','ZAFRA'])\n",
    "X_test_zafra = X_test_zafra.drop(columns=['ABS_IDCOMP','ZAFRA'])\n",
    "y_test_zafra = y_test_zafra.drop(columns=['ABS_IDCOMP','ZAFRA'])\n",
    "\n",
    "print(\"\\nZAFRA Split:\")\n",
    "print(f\"Training set shape: {X_train_zafra.shape}\")\n",
    "print(f\"Testing set shape: {X_test_zafra.shape}\")"
   ],
   "id": "310d585f0900e6bf",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.668463Z",
     "start_time": "2024-09-29T08:43:58.638887Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_zafra",
   "id": "4c9400891e19d3cd",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.683973Z",
     "start_time": "2024-09-29T08:43:58.669484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from sklearn.metrics import accuracy_score, log_loss\n",
    "# \n",
    "# # Function to train and evaluate a model\n",
    "# def train_and_evaluate(X_train, X_test, y_train, y_test, model_name):\n",
    "#     model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "# \n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "#     print(f\"\\n{model_name} Results:\")\n",
    "#     print(f\"Mean Squared Error: {mse}\")\n",
    "#     print(f\"R-squared Score: {r2}\")\n",
    "# \n",
    "# \n",
    "# # Evaluate using 80-20 split\n",
    "# train_and_evaluate(X_train, X_test, y_train, y_test, \"80-20 Split Model\")\n",
    "# \n",
    "# # Evaluate using ZAFRA split\n",
    "# train_and_evaluate(X_train_zafra, X_test_zafra, y_train_zafra, y_test_zafra, \"ZAFRA Split Model\")"
   ],
   "id": "7ef012fd0664f15c",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.698998Z",
     "start_time": "2024-09-29T08:43:58.685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size)\n",
    "        self.fc2 = nn.Linear(input_size, 360)\n",
    "        # self.fc3 = nn.Linear(256, 256)\n",
    "        # self.fc4 = nn.Linear(256, 360)\n",
    "        self.fc5 = nn.Linear(360, 360)\n",
    "        self.fc6 = nn.Linear(360, 512)\n",
    "        self.fc7 = nn.Linear(512, 512)\n",
    "        self.fc8 = nn.Linear(512, 512)\n",
    "        self.fc9 = nn.Linear(512, 360)\n",
    "        self.fc10 = nn.Linear(360, 360)\n",
    "        self.fc11 = nn.Linear(360, 256)\n",
    "        self.fc12 = nn.Linear(256, 128)\n",
    "        self.fc13 = nn.Linear(128, 64)\n",
    "        self.fc14 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.leak = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sig(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sig(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        # x = self.sig(self.fc3(x))\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.sig(self.fc4(x))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.sig(self.fc5(x)) \n",
    "        x = self.dropout(x)\n",
    "        x = self.sig(self.fc6(x))\n",
    "        x = self.sig(self.fc7(x))\n",
    "        x = self.sig(self.fc8(x))\n",
    "        x = self.sig(self.fc9(x))\n",
    "        x = self.sig(self.fc10(x))     \n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc11(x))\n",
    "        x = self.relu(self.fc12(x))\n",
    "        x = self.relu(self.fc13(x))\n",
    "        x = self.fc14(x)\n",
    "        return x\n",
    "    \n",
    "    def l1_loss(self):\n",
    "        l1_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.sum(torch.abs(param))\n",
    "        return l1_loss\n",
    "\n",
    "    def l2_loss(self):\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.sum(param.pow(2))\n",
    "        return l2_loss\n"
   ],
   "id": "d8a56ed20ea6acce",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.714987Z",
     "start_time": "2024-09-29T08:43:58.700980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AutoNET(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(AutoNET, self).__init__()\n",
    "        \n",
    "        output_size = 1  # For regression\n",
    "        \n",
    "        N = 16  # Total number of linear layers (can adjust between 15 and 20)\n",
    "        \n",
    "        # Define the peak size as a multiple of input_size\n",
    "        peak_size = input_size * 4  # Adjust the multiplier as needed\n",
    "        \n",
    "        # Initialize layer sizes list\n",
    "        layer_sizes = []\n",
    "        \n",
    "        # First half layers: increase size\n",
    "        for i in range(N // 2):\n",
    "            size = input_size + int((peak_size - input_size) * (i + 1) / (N // 2))\n",
    "            layer_sizes.append(size)\n",
    "        \n",
    "        # Second half layers: decrease size\n",
    "        for i in range(N // 2):\n",
    "            size = peak_size - int((peak_size - output_size) * (i + 1) / (N // 2))\n",
    "            layer_sizes.append(size)\n",
    "        \n",
    "        # Define the layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_size = input_size\n",
    "        for size in layer_sizes:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "        self.layers.append(nn.Linear(prev_size, output_size))  # Final output layer\n",
    "        \n",
    "        # Activations\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N = len(self.layers)\n",
    "        for i, layer in enumerate(self.layers[:-1]):  # Exclude the last layer\n",
    "            x = layer(x)\n",
    "            \n",
    "            # Activations: sigmoid at the beginning, then leaky ReLU, then ReLU\n",
    "            if i < N // 3:\n",
    "                x = self.sigmoid(x)\n",
    "            elif i < 2 * N // 3:\n",
    "                x = self.leaky_relu(x)\n",
    "            else:\n",
    "                x = self.relu(x)\n",
    "            \n",
    "            # Dropout every two hidden layers\n",
    "            if (i + 1) % 2 == 0:\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        # Final layer without activation (for regression)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "    # L1 loss function\n",
    "    def l1_loss(self):\n",
    "        l1_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.sum(torch.abs(param))\n",
    "        return l1_loss\n",
    "\n",
    "    # L2 loss function\n",
    "    def l2_loss(self):\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.sum(param.pow(2))\n",
    "        return l2_loss\n"
   ],
   "id": "c2ccf32019a5bb5",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.746992Z",
     "start_time": "2024-09-29T08:43:58.715985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "X_train = X_train_zafra\n",
    "y_train = y_train_zafra\n",
    "X_test = X_test_zafra\n",
    "y_test = y_test_zafra\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1)\n",
    "\n",
    "X_train = X_train_tensor \n",
    "y_train = y_train_tensor\n",
    "X_test = X_test_tensor\n",
    "y_test = y_test_tensor"
   ],
   "id": "49c1a41e2188b323",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.793021Z",
     "start_time": "2024-09-29T08:43:58.748997Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_zafra",
   "id": "49edcea35037a441",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:58.808024Z",
     "start_time": "2024-09-29T08:43:58.795021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = X_train.shape[1]\n",
    "input_size"
   ],
   "id": "f08e10b1840c772e",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:59.866856Z",
     "start_time": "2024-09-29T08:43:58.810023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Función para calcular el coeficiente de determinación (R^2)\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    ss_res = ((y_true - y_pred) ** 2).sum()\n",
    "    ss_tot = ((y_true - y_true.mean()) ** 2).sum()\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2.item()\n",
    "\n",
    "# Definir el modelo, el criterio y el optimizador\n",
    "input_size = X_train.shape[1]\n",
    "model = Net(input_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "l1_lambda = 0.01\n",
    "l2_lambda = 0.01\n",
    "\n",
    "\n",
    "# Función de entrenamiento"
   ],
   "id": "1dfe60a2c8a81b06",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:59.881834Z",
     "start_time": "2024-09-29T08:43:59.867835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # create a nn class (just-for-fun choice :-) \n",
    "# class RMSELoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.mse = nn.MSELoss()\n",
    "#         \n",
    "#     def forward(self,yhat,y):\n",
    "#         return torch.sqrt(self.mse(yhat,y))\n",
    "# \n",
    "# criterion = RMSELoss()"
   ],
   "id": "488185aeb259ff70",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:43:59.896841Z",
     "start_time": "2024-09-29T08:43:59.882833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, X_train, y_train, X_test, y_test, num_epochs=100):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_r2s = []\n",
    "    test_r2s = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        outputs = model(X_train)\n",
    "        # train_loss =  torch.sqrt(criterion(outputs, y_train))\n",
    "        train_loss =  criterion(outputs, y_train)\n",
    "        \n",
    "        l1_reg = l1_lambda * model.l1_loss()\n",
    "        l2_reg = l2_lambda * model.l2_loss()\n",
    "        train_loss = train_loss + l1_reg + l2_reg\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Actualizar el learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluación en test\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        # Calcular R^2 como métrica de precisión\n",
    "        train_r2 = calculate_r2(y_train, outputs)\n",
    "        test_r2 = calculate_r2(y_test, test_outputs)\n",
    "        train_r2s.append(train_r2)\n",
    "        test_r2s.append(test_r2)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], LR: {scheduler.get_last_lr()[0]:.6f}, Train Loss: {train_loss.item():.4f}, Test Loss: {test_loss.item():.4f}, Train R^2: {train_r2:.4f}, Test R^2: {test_r2:.4f}')\n",
    "\n",
    "    return train_losses, test_losses, train_r2s, test_r2s"
   ],
   "id": "80511606c4f9640b",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:44:43.134768Z",
     "start_time": "2024-09-29T08:43:59.897842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Entrenar el modelo\n",
    "num_epochs = 100\n",
    "train_losses, test_losses, train_r2s, test_r2s = train_model(model, criterion, optimizer,scheduler, X_train, y_train, X_test, y_test, num_epochs=num_epochs)\n",
    "\n",
    "# Guardar las pérdidas en un archivo\n",
    "losses_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Train Loss': train_losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Train R^2': train_r2s,\n",
    "    'Test R^2': test_r2s\n",
    "})\n",
    "losses_df.to_csv('training_losses.csv', index=False)\n",
    "\n",
    "print('Training complete. Losses and accuracies saved to training_losses.csv')\n",
    "\n",
    "# Graficar las pérdidas a través de las épocas\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Graficar el R^2 a través de las épocas\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_r2s, label='Train R^2')\n",
    "plt.plot(range(1, num_epochs + 1), test_r2s, label='Test R^2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R^2')\n",
    "plt.legend()\n",
    "plt.title('R^2 over Epochs')\n",
    "plt.show()"
   ],
   "id": "c06630a0dd422424",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:37:08.744265Z",
     "start_time": "2024-09-29T08:37:08.715598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).cpu().detach().numpy()\n",
    "\n",
    "# Convertir y_test a numpy array si es necesario\n",
    "y_test_np = y_test.cpu().detach().numpy()\n",
    "\n",
    "# Calcular MSE, RMSE, MAE y R^2\n",
    "mse = mean_squared_error(y_test_np, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_np, predictions)\n",
    "r2 = r2_score(y_test_np, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R²): {r2}\")"
   ],
   "id": "a33cd371d1bf7cc",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:36:13.651758Z",
     "start_time": "2024-09-29T08:36:13.460099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).cpu().detach().numpy()\n",
    "    y_test_np = y_test.cpu().detach().numpy()  # Convertir y_test a Numpy array\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.scatter(y_test_np, predictions, alpha=0.5, color='blue', label='Predicciones')\n",
    "\n",
    "# Gráfico de dispersión para valores reales\n",
    "plt.scatter(y_test_np, y_test_np, alpha=0.5, color='red', label='Valores reales')\n",
    "plt.plot([y_test_np.min(), y_test_np.max()], [y_test_np.min(), y_test_np.max()], 'r', lw=2)\n",
    "\n",
    "plt.title('Predicted vs Actual Values')\n",
    "plt.show()"
   ],
   "id": "b1c40c15b9d61b96",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T01:24:27.780564Z",
     "start_time": "2024-07-24T01:24:25.473346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "rendimiento = data['rendimiento']\n",
    "\n",
    "# Transformaciones\n",
    "log_transformed = np.log(rendimiento)\n",
    "sqrt_transformed = np.sqrt(rendimiento)\n",
    "\n",
    "# Crear figura y ejes\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Histograma de datos originales\n",
    "sns.histplot(rendimiento, bins=30, kde=True, ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Original')\n",
    "\n",
    "# Q-Q plot de datos originales\n",
    "stats.probplot(rendimiento, dist=\"norm\", plot=axs[0, 1])\n",
    "axs[0, 1].set_title('Original: Q-Q plot')\n",
    "\n",
    "# Histograma de datos transformados con logaritmo\n",
    "sns.histplot(log_transformed, bins=30, kde=True, ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Log Transformation')\n",
    "\n",
    "# Histograma de datos transformados con raíz cuadrada\n",
    "sns.histplot(sqrt_transformed, bins=30, kde=True, ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Sqrt Transformation')\n",
    "\n",
    "plt.suptitle('Normality Diagnosis Plot (rendimiento)')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ],
   "id": "6f99b3001afc27b5",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T00:11:37.796443Z",
     "start_time": "2024-08-24T00:11:37.778569Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), '6monthNN.pth')\n",
   "id": "180e783e259d4da3",
   "execution_count": 24,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
